{
  "nav": {
    "articles": "Articles",
    "card": "Business Card"
  },
  "header": {
    "name": "Anas Hamad",
    "role": "AI & Machine Learning Engineer",
    "location": "Amman, Jordan",
    "verified": "Verified",
    "verifiedTooltip": "This person is trusted by a lot of people :)"
  },
  "topTechStack": [
    "Python",
    "LLMs",
    "NLP",
    "Data Manipulation",
    "TensorFlow",
    "RAG",
    "Fine Tuning"
  ],
  "description": [
    "Hi! I'm an <u>AI and Machine Learning Engineer</u> with a degree in <u>Computer Engineering</u>. I like building things that make life easier using <u>data</u> and <u>AI</u>. Some of my favorite projects are <u>Onqoud</u>, which helps restaurants understand their business better, and <u>ElmLLM</u>, an <u>Arabic AI model</u> I built from scratch. I usually work with <u>Python</u>, <u>TensorFlow</u>, and tools like <u>Power BI</u> to show data in a clear way.",
    "I've won <u>1st place</u> in a <u>NASA competition</u> and was part of the top <u>AI project</u> at <u>PSUT</u>. I'm always looking to <u>learn more</u>, <u>build cool ideas</u>, and <u>connect</u> with people who love working on <u>smart and helpful tech</u>."
  ],
  "cta": {
    "contact": "Contact me",
    "resume": "View Resume"
  },
  "sections": {
    "experience": "EXPERIENCE",
    "education": "EDUCATION",
    "tech_stack": "TECH STACK",
    "achievements": "ACHIEVEMENTS",
    "certifications": "CERTIFICATIONS & COURSES",
    "projects": "PROJECTS",
    "articles": "ARTICLES",
    "social": "SOCIAL MEDIA",
    "view_all": "View all"
  },
  "experiences": [
    {
      "role": "AI Product Development",
      "company": "Onqoud",
      "logo": "https://avatars.githubusercontent.com/u/198729474?s=200&v=4",
      "period": "2024 - present",
      "description": "Led development of a scalable, AI-driven SaaS platform for restaurants and cafés, automating data pipelines and generating 50+ dynamic Python reports. Optimized operations with predictive analytics, increasing performance metrics by 30% and cutting operational costs by 25%."
    },
    {
      "role": "Data Scientist & AI Operations",
      "company": "Digizag",
      "logo": "https://images.crunchbase.com/image/upload/c_pad,h_256,w_256,f_auto,q_auto:eco,dpr_1/q5gghnkvomhp60df5jse",
      "period": "2025 - present",
      "description": "Automated 80% of daily data processing across 1M+ records using Python (Pandas, NumPy) and Airflow, saving 60 hours of manual work weekly. Fine-tuned domain LLMs to 90% accuracy and delivered three ML models (XGBoost, LSTM) that boosted prediction accuracy by 20%."
    },
    {
      "role": "AI & Data Science Intern",
      "company": "ShAI",
      "logo": "https://www.shai.sa/assets/logo.svg",
      "period": "2024 - 2024",
      "description": "Engineered ensemble classifiers with Random Forest on 500K+ customer records, reaching 89% accuracy on imbalanced sets via SMOTE (20% oversampling) and 5:1 class weighting. Preprocessed data with Pandas/NumPy, imputing 15% missing values (KNN k=5), removing 2% outliers via IQR, and normalizing 10+ features."
    },
    {
      "role": "Python Developer Intern",
      "company": "Ahli Bank",
      "logo": "https://ahli.com/wp-content/uploads/2024/10/logo.svg?x57988",
      "period": "2023 - 2023",
      "description": "Designed custom in-memory Python data structures for financial transactions, cutting batch runtimes by 40% (50 → 30 minutes). Built an AWS Lambda serverless pipeline ingesting 100GB+ of data daily with 99.9% uptime and 20% lower storage costs."
    }
  ],
  "education": [
    {
      "institution": "Balqaa Applied University",
      "degree": "Bachelor of Engineer in Computer Engineering",
      "period": "2020 - 2025",
      "description": "Focused on software development, data structures, machine learning, and AI systems. Learned how to build smart solutions using both hardware and software."
    }
  ],
  "achievements": [
    {
      "title": "PSUT Best Startup Idea",
      "issuer": "PSUT",
      "period": "2023",
      "description": "Fund for winning the best startup idea"
    },
    {
      "title": "First place NASA AI competition",
      "issuer": "NASA",
      "period": "2024",
      "description": "First place in NASA competition for best project in artificial intelligence."
    },
    {
      "title": "Best Orange Incubator Project",
      "issuer": "Orange",
      "period": "2022-2023",
      "description": "6-month journey at Orange Incubator learning AI startup product development."
    },
    {
      "title": "Hackathon Winner - AI",
      "issuer": "ACM Jordan",
      "period": "2023",
      "description": "Led a team to victory in the AI category by developing a real-time sentiment analysis tool for social media monitoring."
    }
  ],
  "certifications": [
    {
      "title": "Data Science & AI Track",
      "issuer": "Data Camp",
      "period": "2025",
      "description": "Comprehensive course covering neural networks, CNNs, RNNs, and deep learning best practices"
    },
    {
      "title": "Machine Learning Engineering Track",
      "issuer": "Data Camp",
      "period": "2024",
      "description": "Advanced certification covering ML implementation on AWS cloud infrastructure."
    },
    {
      "title": "Cloud Professional For Engineers",
      "issuer": "Coursera",
      "period": "2023",
      "description": "Advanced Skills to AWS Cloud and implementation."
    }
  ],
  "projects": [
    {
      "title": "ElmLLM",
      "description": "ElmLLM is a Arabic LLM trained on data from our culture. It is designed to understand and generate Arabic text, making it a valuable tool for various applications.",
      "tech": [
        "Python",
        "PyTorch",
        "Transformers",
        "Hugging Face",
        "CUDA",
        "Arabic NLP"
      ],
      "github": "https://github.com/anashamad9/ElmLLM",
      "demo": "https://github.com/anashamad9/ElmLLM",
      "period": "2025"
    },
    {
      "title": "Onqoud AI",
      "description": "Developed a BI product with features like future forecasting and insight for restaurants.",
      "tech": [
        "Python",
        "PowerBI",
        "Tensorflow",
        "Next.js",
        "Tableau",
        "AWS"
      ],
      "github": "https://github.com/onqoud",
      "demo": "https://www.onqoud.com",
      "period": "2025"
    },
    {
      "title": "Search Recommendation Engine",
      "description": "Created a machine learning recommendation system achieving 85% accuracy using collaborative filtering.",
      "tech": [
        "Python",
        "Scikit-learn",
        "Pandas",
        "Flask",
        "Collaborative Filtering"
      ],
      "github": "https://github.com/username/recommendation-engine",
      "demo": "https://recommendation-engine.demo.com",
      "period": "2022"
    }
  ],
  "articles": [
    {
      "id": 10,
      "enabled": true,
      "topic": "How llms.txt Makes Your Life Better",
      "date": "April 27, 2025",
      "readTime": "12 min read",
      "content": "The Awesome llms.txt: A Revolution in AI Content Discovery and Answer Engine Optimization (AEO)\n\nImagine a digital world where artificial intelligence (AI) can grasp the essence of your website in moments, delivering precise answers to users without getting lost in a maze of webpages, ads, or sidebar links. Sounds awesome, right? That’s exactly what llms.txt does—an innovative proposal that’s sparked a revolution in how large language models (LLMs) like ChatGPT, Claude, and others interact with websites. Launched by AI researcher Jeremy Howard in September 2024, this simple yet powerful Markdown file is set to transform AI-driven content discovery and boost Answer Engine Optimization (AEO). Get ready, because this article is gonna get you hyped about llms.txt and its game-changing potential!\n\nWhat Is llms.txt?\n\nThe internet wasn’t designed for AI. It’s a chaotic mess of HTML, JavaScript, and ads that make it tough for large language models (LLMs) to navigate efficiently. Enter llms.txt, a plain-text Markdown file placed at the root of a website (like https://example.com/llms.txt). Think of it as a custom-made guide for AI models, shouting, “Here’s the important stuff!” Unlike robots.txt, which tells search engine crawlers what to avoid, or sitemaps, which list pages for indexing, llms.txt is built specifically for large language models. It curates carefully selected content like documentation, APIs, or FAQs, presented in a clean, easy-to-parse format.\n\nWhy’s it so awesome? Because LLMs have a limited context window, making it hard to process massive websites without wasting tokens (more tokens = more time and energy) on irrelevant stuff like navigation bars. With llms.txt, website owners can provide a concise, AI-friendly summary or links to key pages, reducing computational load and boosting answer accuracy. It’s like a cheat sheet for your site’s best content!\n\nA New Standard Is Born\n\nThe llms.txt proposal is the brainchild of Jeremy Howard, a big name in the AI world. He noticed that with AI-powered searches expected to hit 10% of total internet traffic by late 2025, websites need a new way to shine in the AI era. His solution? A lightweight Markdown file that’s easy to implement and accessible to all. Companies like Cursor, Anthropic, and Mintlify are already jumping on board, and the excitement is contagious. This isn’t just a tech tweak; it’s a radical shift that could make your site a go-to source for AI-generated answers.\n\nWhat makes llms.txt so cool is its simplicity. No complex rules or proprietary formats. Any developer can whip up an llms.txt file in minutes, and LLMs can read it effortlessly. It’s a win-win: site owners control how their content is presented to AI, and users get faster, more accurate answers.\n\nHow llms.txt Works: A Peek Under the Hood\n\nLet’s dive into the details, because the way llms.txt works is straight-up awesome. The file lives at your site’s root (/llms.txt) and is written in Markdown, a format LLMs love for its clean syntax. The structure is flexible but usually includes sections like:\n\nHomepage: A brief overview of your site’s purpose.\n\nDocs: Links or snippets from key documentation (like APIs or user guides).\n\nOptional: Secondary content like pricing pages, blogs, or FAQs.\n\nHere’s a sample llms.txt file to get you pumped:\n\nllms.txt for AwesomeDevTools\nHomepage\nWelcome to AwesomeDevTools, the ultimate platform for building AI-powered apps with seamless integrations.\n\nDocs\nQuickstart Guide: https://awesomedevtools.com/docs/quickstart.md\nAPI Reference: https://awesomedevtools.com/docs/api-reference.md\nTroubleshooting: https://awesomedevtools.com/docs/troubleshooting.md\nOptional\nPricing: https://awesomedevtools.com/pricing.md\nBlog: https://awesomedevtools.com/blog/index.md\nThis file is pure gold for LLMs, guiding them to the most important content without wrestling with complex HTML. Some sites even offer a /llms-full.txt version with complete content for deeper dives. The best part? It’s all optional and customizable, giving you total control over what AI sees.\n\nllms.txt and AEO: The Perfect Duo\n\nNow, let’s talk about why llms.txt is a game-changer for Answer Engine Optimization (AEO). AEO is the art of optimizing content to rank high in AI-powered answer engines like ChatGPT or Grok. Unlike traditional SEO, which focuses on page rankings, AEO aims to make your content the primary source for direct, accurate answers in AI responses. With AI-driven searches projected to reach 15–20% of internet traffic by 2026, AEO is the next big thing, and llms.txt is your secret weapon.\n\nWhy is llms.txt perfect for AEO? It’s all about control and clarity:\n\nDirect Answers: By curating key content in llms.txt, you ensure LLMs prioritize your main pages, increasing their chances of appearing in AI answers.\n\nLess Noise: LLMs can get sidetracked by irrelevant site elements (like ads or old posts). llms.txt cuts through the clutter, giving AI exactly what users need.\n\nStructured Data: The Markdown format aligns with how LLMs process queries, making it easier to extract and summarize content accurately.\n\nCompetitive Edge: Sites using llms.txt are more likely to show up in AI answers, driving traffic and boosting brand visibility.\n\nFor example, if a user asks, “How do I integrate AwesomeDevTools’ API?”, an LLM can grab the API reference link from llms.txt and deliver a spot-on answer in seconds, crediting your site. Without llms.txt, it might miss the mark, pulling from a less relevant page or a competitor.\n\nDoes Adding llms.txt Require Retraining the LLM?\n\nOne question I’ve been thinking about: does adding an llms.txt file mean you need to retrain a large language model? Short answer: Nope, and that’s what makes llms.txt so awesome! LLMs like Grok or ChatGPT don’t need retraining to use llms.txt. The file is designed as an external interface that models can read during runtime when fetching info from your site. When /llms.txt is detected, the model can parse its content directly, using the links or snippets to form answers, without any tweaks to its core training.\n\nThere’s a small catch, though: for LLMs to leverage llms.txt, they need to be programmed to look for it or rely on web crawling systems that support this standard. Since llms.txt is still a new proposal (from September 2024), some models might not check for it yet, but rest assured, it’s likely the first file they’ll read as adoption grows.\n\nPython Code to Generate llms.txt\n\nIf you wanna whip up an llms.txt file fast (though I’d recommend crafting it by hand for that personal touch), here’s a Python script that generates one for your website. It pulls key pages from a predefined list and formats them into a Markdown file ready to deploy:\n\nimport os\n\ndef generate_llms_txt(site_name, homepage_desc, docs, optional=None):\n\"\"\"\nGenerate an llms.txt file for a website.\n\nArgs:\n    site_name (str): Name of the website\n    homepage_desc (str): Brief description of the site\n    docs (list): List of tuples (title, URL) for documentation links\n    optional (list, optional): List of tuples (title, URL) for optional links\n\"\"\"\n# Start building the Markdown content\ncontent = f\"# llms.txt for {site_name}\n\n\"\n\n# Add Homepage section\ncontent += \"## Homepage\n\"\ncontent += f\"{homepage_desc}\n\n\"\n\n# Add Docs section\ncontent += \"## Docs\n\"\nfor title, url in docs:\n    content += f\"- {title}: {url}\n\"\n\n# Add Optional section if provided\nif optional:\n    content += \"\n## Optional\n\"\n    for title, url in optional:\n        content += f\"- {title}: {url}\n\"\n\n# Write to llms.txt\nwith open(\"llms.txt\", \"w\") as f:\n    f.write(content)\nprint(f\"Generated llms.txt for {site_name}\")\nExample usage\nsite = \"AwesomeDevTools\"\ndescription = \"Welcome to AwesomeDevTools, the ultimate platform for building AI-powered apps with seamless integrations.\"\ndocs = [\n(\"Quickstart Guide\", \"https://awesomedevtools.com/docs/quickstart.md\"),\n(\"API Reference\", \"https://awesomedevtools.com/docs/api-reference.md\"),\n(\"Troubleshooting\", \"https://awesomedevtools.com/docs/troubleshooting.md\")\n]\noptional = [\n(\"Pricing\", \"https://awesomedevtools.com/pricing.md\"),\n(\"Blog\", \"https://awesomedevtools.com/blog/index.md\")\n]\n\ngenerate_llms_txt(site, description, docs, optional)\n\nThis script creates an llms.txt file like the one above and saves it to your project directory. You can customize the site_name, homepage_desc, docs, and optional variables to match your site’s content.\n\nWhy llms.txt Matters for AEO\n\nHigher Answer Accuracy: LLMs are more likely to cite your exact content, driving targeted traffic.\n\nBrand Authority: Consistent, accurate citations in AI responses build trust and influence.\n\nScalability: As AI-powered search grows, llms.txt keeps your site in the game.\n\nThe Adoption Wave: Who’s Using llms.txt?\n\nThe llms.txt movement is picking up steam fast. Companies like Cursor (an AI-powered code editor), Anthropic (makers of Claude), and Mintlify (a documentation platform) have already adopted it. For example, Mintlify’s llms.txt might look like this:\n\nllms.txt for Mintlify\nHomepage\nMintlify: Beautiful, easy-to-maintain documentation for developers.\n\nDocs\nGetting Started: https://mintlify.com/docs/getting-started.md\nAPI Docs: https://mintlify.com/docs/api.md\nCustomization: https://mintlify.com/docs/customization.md\nOptional\nPricing: https://mintlify.com/pricing\nBlog: https://mintlify.com/blog\nThis file ensures LLMs quickly grasp Mintlify’s core offerings, boosting its chances of appearing in AI responses. The result? More traffic, better user experiences, and a stronger brand presence.\n\nJoin the Movement\n\nllms.txt isn’t just a file; it’s a call to build a smarter, AI-ready internet and a cornerstone for AEO. It’s your chance to make your content shine in a world where AI is the new search engine. Whether you’re a solo developer or a big company, llms.txt levels the playing field, giving everyone a shot at AI stardom. So, what are you waiting for? Create your llms.txt today and let’s shape the future of the web together!\n\nFor the latest updates, check out llmstxt.org. The AI revolution is here, and llms.txt is your front-row ticket. Let’s make the internet awesome and AEO-ready!\n"
    },
    {
      "id": 9,
      "enabled": true,
      "topic": "llms.txt: Teaching LLMs to Behave",
      "date": "April 26, 2025",
      "readTime": "16 min read",
      "content": "# llms.txt: A Tiny Text File That Teaches Big Models How To Behave\nImagine if every website could tell AI models exactly how to use its content, in plain words a machine can follow.\nThat is the promise of llms.txt: a small text file at your site’s root that acts like a handshake between your content and any Large Language Model that wants to read it.\nIn this article you will get:\n- A clear definition and why it matters right now.\n- A practical mini spec you can copy.\n- Example files for permissive, restrictive, and paid-access sites.\n- A small Python code block an AI crawler could use to honor llms.txt.\n- A detailed flow graph describing how an LLM agent should read and obey llms.txt.\n- A full section on business growth with AEO, plus an AEO-optimized llms.txt template and checklist.\n\n## 1) Why llms.txt now?\nAI models learn from the open web, but the web was never designed with LLMs in mind. We already have robots.txt for search crawlers, and some sites publish signals like ai.txt or HTTP headers. Still, brands and creators want finer control.\n\nQuestions teams ask every week:\n- Can AI summarize our articles but avoid long quotes?\n- Can it answer questions using our docs if it links back?\n- Can it cache our pages, and for how long?\n- Should it call an API or pay instead of scraping?\n- What is the safe rate so our server does not melt at night?\n\nllms.txt is a simple convention that answers those questions in one predictable place.\n\n## 2) What is llms.txt?\nllms.txt is a plain-text policy file you publish at the root of your domain: `https://yourdomain.com/llms.txt`.\n\nIt tells respectful LLM agents:\n- Who the rules apply to using `User-Agent` lines.\n- Where they can and cannot go using `Allow` and `Disallow`.\n- How fast they may fetch using `Crawl-Delay` and `Rate-Limit`.\n- How they may reuse content, such as summarize only or quote limits, and whether training is allowed.\n- Whether they must use an API, license, or payment link.\n- How to attribute and where to send usage reports.\n\nThink of it as robots.txt, but AI aware. It does not replace robots.txt. A respectful agent should honor both and use the stricter rule when they conflict.\n\n## 3) Design principles\n**Plain text first** – Human readable and machine parseable. Comments allowed. UTF-8.\n\n**Layered controls** – Defaults at the top, then overrides per `User-Agent` or path. Most specific wins.\n\n**Fail safe** – If the file is missing or malformed, agents fall back to robots.txt and conservative defaults.\n\n**Transparency** – Put contact info, effective date, and a short summary at the top.\n\n**Consent over scraping** – Prefer linking to an API or license page when possible. Make good behavior easy.\n\n## 4) The mini spec\nThis is a practical starter format many agents can follow.\n\n- `#` starts a comment line.\n- `User-Agent`: exact name or wildcard like `LLM-*` or `*`.\n- `Allow`: path or glob pattern.\n- `Disallow`: path or glob pattern.\n- `Crawl-Delay`: seconds between requests.\n- `Rate-Limit`: requests per minute, optionally per path.\n- `Cache-TTL`: seconds content may be cached.\n- `Reuse-Policy`: one or more rules such as `summarize-only`, `quote-max: N` words, `training-allowed: true|false`.\n- `Attribution`: required fields such as link, title, author, site.\n- `Cite-URL`: canonical URL template to cite.\n- `License`: link to your license, or `none`.\n- `Paid-Use`: link to paid terms or checkout for AI usage.\n- `API`: first-party API agents should call instead of scraping.\n- `Report-Usage`: endpoint or email for usage reports.\n- `Contact`: human contact for questions.\n- `Effective-Date`: policy date.\n- **Order**: top-level defaults first, then one or more `User-Agent` blocks. The most specific matching block wins.\n\n## 5) Example llms.txt files\n**A) Permissive site with attribution and quote limits**\n```text\n# llms.txt for example.org\n# We welcome responsible AI use. Please cite and keep quotes short.\n\nEffective-Date: 2025-10-19\nContact: ai@example.org\nCite-URL: https://example.org{path}\nAttribution: link,title,site\nLicense: https://example.org/license\nAPI: https://api.example.org/v1/docs\nReport-Usage: https://example.org/ai-usage\nCache-TTL: 86400\nReuse-Policy: summarize-only, quote-max: 100 words, training-allowed: true\n\nUser-Agent: *\nAllow: /\nDisallow: /admin/\nDisallow: /checkout/\nCrawl-Delay: 2\nRate-Limit: 30 rpm\n```\n\n**B) Restrictive site: summaries allowed, training forbidden**\n```text\n# llms.txt for news.example\nEffective-Date: 2025-10-19\nContact: newsroom@example.com\nCite-URL: https://news.example{path}\nAttribution: link,title,author\nLicense: none\nCache-TTL: 3600\nReuse-Policy: summarize-only, quote-max: 60 words, training-allowed: false\n\nUser-Agent: LLM-*\nAllow: /stories/\nDisallow: /premium/\nDisallow: /photos/\nCrawl-Delay: 5\nRate-Limit: 10 rpm\n```\n\n**C) Paid usage only, via API**\n```text\n# llms.txt for docs.vendor.com\nEffective-Date: 2025-10-19\nContact: devrel@vendor.com\nPaid-Use: https://vendor.com/ai-pricing\nAPI: https://api.vendor.com/v2/search\nReuse-Policy: summarize-only, quote-max: 50 words, training-allowed: false\n\nUser-Agent: *\nDisallow: /\nAllow: /public/faq/\nCrawl-Delay: 10\nRate-Limit: 5 rpm\n```\n\n## 6) A tiny Python snippet an LLM crawler could run\nThis compact snippet shows how an agent might fetch and honor llms.txt. It handles wildcards, falls back to robots.txt, and returns an actionable policy.\n```python\nimport re\nimport time\nfrom urllib.parse import urljoin, urlparse\nimport requests\n\nclass LlmsPolicy:\n    def __init__(self):\n        self.allow = []\n        self.disallow = []\n        self.crawl_delay = 0\n        self.rate_limit_rpm = None\n        self.cache_ttl = None\n        self.reuse = {}\n        self.cite_url = None\n        self.attrib = None\n        self.license = None\n        self.paid_use = None\n        self.api = None\n        self.report_usage = None\n\n    def is_allowed(self, path: str) -> bool:\n        def match(pattern, s):\n            regex = \"^\" + re.escape(pattern).replace(\"\\*\", \".*\").replace(\"\\?\", \".\") + \"$\"\n            return re.match(regex, s) is not None\n\n        candidates = []\n        for p in self.allow:\n            if match(p, path):\n                candidates.append((\"allow\", p))\n        for p in self.disallow:\n            if match(p, path):\n                candidates.append((\"disallow\", p))\n        if not candidates:\n            return True\n        rule = sorted(candidates, key=lambda x: len(x[1]), reverse=True)[0][0]\n        return rule == \"allow\"\n\ndef fetch_llms_txt(base_url: str):\n    for name in [\"llms.txt\", \"ai.txt\", \"robots.txt\"]:\n        url = urljoin(base_url, \"/\" + name)\n        try:\n            r = requests.get(url, timeout=5)\n            if r.status_code == 200 and r.text.strip():\n                return name, r.text\n        except requests.RequestException:\n            continue\n    return None, None\n\ndef parse_llms_txt(text: str, agent: str) -> LlmsPolicy:\n    blocks = {\"*\": {}}\n    current = \"*\"\n    for line in text.splitlines():\n        line = line.strip()\n        if not line or line.startswith(\"#\"):\n            continue\n        if line.lower().startswith(\"user-agent:\"):\n            current = line.split(\":\", 1)[1].strip()\n            blocks.setdefault(current, {})\n            continue\n        key, val = [x.strip() for x in line.split(\":\", 1)]\n        blocks[current].setdefault(key, []).append(val)\n\n    def best_block():\n        if agent in blocks:\n            return blocks[agent]\n        for name in blocks.keys():\n            if \"*\" in name:\n                pattern = \"^\" + re.escape(name).replace(\"\\*\", \".*\") + \"$\"\n                if re.match(pattern, agent):\n                    return blocks[name]\n        return blocks.get(\"*\", {})\n\n    chosen = best_block()\n    pol = LlmsPolicy()\n\n    def get1(k):\n        return chosen.get(k, [None])[-1]\n\n    for a in chosen.get(\"Allow\", []):\n        pol.allow.append(a)\n    for d in chosen.get(\"Disallow\", []):\n        pol.disallow.append(d)\n    cd = get1(\"Crawl-Delay\")\n    if cd:\n        pol.crawl_delay = int(re.findall(r\"\\d+\", cd)[0])\n    rl = get1(\"Rate-Limit\")\n    if rl:\n        m = re.search(r\"(\\d+)\\s*rpm\", rl, re.I)\n        if m:\n            pol.rate_limit_rpm = int(m.group(1))\n    ct = get1(\"Cache-TTL\")\n    if ct:\n        pol.cache_ttl = int(re.findall(r\"\\d+\", ct)[0])\n    pol.cite_url = get1(\"Cite-URL\")\n    pol.attrib = get1(\"Attribution\")\n    pol.license = get1(\"License\")\n    pol.paid_use = get1(\"Paid-Use\")\n    pol.api = get1(\"API\")\n    pol.report_usage = get1(\"Report-Usage\")\n\n    reuse = get1(\"Reuse-Policy\")\n    if reuse:\n        for token in reuse.split(\",\"):\n            token = token.strip()\n            if token == \"summarize-only\":\n                pol.reuse[\"summarize_only\"] = True\n            elif token.startswith(\"quote-max\"):\n                m = re.search(r\"quote-max:\\s*(\\d+)\", token)\n                if m:\n                    pol.reuse[\"quote_max\"] = int(m.group(1))\n            elif token.startswith(\"training-allowed\"):\n                m = re.search(r\"training-allowed:\\s*(true|false)\", token, re.I)\n                if m:\n                    pol.reuse[\"training_allowed\"] = m.group(1).lower() == \"true\"\n\n    return pol\n\ndef polite_get(url: str, policy: LlmsPolicy, last_fetch_time: dict):\n    host = urlparse(url).netloc\n    since = time.time() - last_fetch_time.get(host, 0)\n    wait = max(0, policy.crawl_delay - since)\n    if wait > 0:\n        time.sleep(wait)\n    resp = requests.get(url, timeout=10)\n    last_fetch_time[host] = time.time()\n    return resp\n\nif __name__ == \"__main__\":\n    base = \"https://example.org\"\n    agent_name = \"LLM-Reader/1.0\"\n    name, text = fetch_llms_txt(base)\n    policy = parse_llms_txt(text, agent_name) if text else LlmsPolicy()\n\n    page = \"https://example.org/stories/how-to-build\"\n    if policy.is_allowed(urlparse(page).path):\n        last = {}\n        r = polite_get(page, policy, last)\n        print(\"Fetched:\", r.status_code)\n    else:\n        print(\"Path disallowed by policy.\")\n```\nYou can expand this to handle per-path limits, send usage reports, or switch to API calls if scraping is disallowed.\n\n## 7) The flow graph: how an LLM should read and obey llms.txt\n```mermaid\nflowchart LR\n  subgraph A[Discovery]\n    A1[Start: Agent intends to use URL U]\n    A2[Derive origin (scheme://host/)]\n    A3[Fetch llms.txt → ai.txt → robots.txt]\n    A4{Policy text found?}\n    A5[Apply conservative defaults\n      Allow /\n      Disallow /admin, /checkout\n      Crawl-Delay 5s\n      Reuse summarize-only\n      quote-max 50 words\n      training forbidden]\n  end\n  subgraph B[Policy Resolution]\n    B1[Parse file, strip comments]\n    B2[Select best User-Agent block\n        exact → wildcard → *]\n    B3[Merge block with defaults]\n    B4[Build enforcement plan\n        Allow/Disallow patterns\n        Crawl-Delay & Rate-Limit timers\n        Reuse, Cite-URL, License, API, Paid-Use, Cache-TTL\n        Side plans: API access, payment]\n  end\n  subgraph C[Enforcement]\n    C1{Is URL allowed by longest match?}\n    C2[Respect Crawl-Delay & Rate-Limit]\n    C3[Fetch content with GET]\n    C4[Enforce Reuse-Policy\n        summarize-only, quote-max, training flag]\n    C5[Build citation (Cite-URL + Attribution)]\n    C6[Cache using Cache-TTL]\n    C7[Report usage if endpoint exists]\n    C8[Done: return answer with attribution]\n  end\n\n  A1 --> A2 --> A3 --> A4\n  A4 -->|Yes| B1\n  A4 -->|No| A5 --> B1\n  B4 --> C1\n  C1 -->|Yes| C2\n  C1 -->|No & API| D1[Call API] --> C2\n  C1 -->|No & no API| D2[Abort respectfully]\n  C2 --> C3 --> C4 --> C5 --> C6 --> C7 --> C8\n\n  style A4 fill:#ffe5e5\n  style C1 fill:#ffe5e5\n```\n\n## 8) How llms.txt helps businesses grow and win in AEO\nAEO means optimizing your brand so AI answer engines choose your content when users ask questions. llms.txt gives you a direct line to those engines. It tells respectful agents what they may use, how to cite you, and where to link users who want more. That control turns AI answers into traffic, leads, and trust.\n\n**Be eligible and visible in answers** – When an LLM may summarize your content and is told how to cite you, your brand is more likely to appear inside the answer box. Clear rules raise crawler confidence, which improves inclusion.\n\n**Win the click with clean attribution** – Use `Attribution` and `Cite-URL` to shape how your brand appears. Require a live link and a readable title. This makes the answer feel official and pushes more users to your site.\n\n**Channel high intent users to the right page** – Point `API` to your product or search API. Point `Cite-URL` to canonical pages with UTM parameters. If a user asks for pricing, the answer engine can call your API or cite your pricing page. That means fewer stale numbers and more qualified visits.\n\n**Protect your value and still gain reach** – Set `Reuse-Policy` to summarize only with a quote-max. AI can explain your ideas, while long form detail stays on your site. Users who want depth will click through.\n\n**Build brand signals engines trust** – Add a top-of-file summary, a contact, a license, and an effective date. These are small trust signals. Paired with strong schema.org on your pages, they help AEO systems link your brand to quality.\n\n**Measure and learn from AI traffic** – Use `Report-Usage` so cooperative agents send a small JSON log. Combine with UTM in `Cite-URL`. You will see which questions your brand already answers and which pages convert after AI exposure.\n\n**Control freshness and reduce churn** – Set `Cache-TTL`. For prices, inventory, or time sensitive posts, a short TTL reduces stale answers. Fresh answers build trust and clicks.\n\n**Avoid overload and stay indexable** – Use `Crawl-Delay` and `Rate-Limit` to spread AI visits across time. A fast site gets crawled more and cited more.\n\n**Help the model tell your story correctly** – Link an API or product feed for facts such as price, SKU, and availability. Link short FAQs for quick answers. The cleaner the source, the cleaner the answer that carries your brand.\n\n## 9) AEO-optimized llms.txt you can copy\n```text\n# llms.txt for yourbrand.com\n# Goal: make AI answers cite us correctly and send users to the right pages.\n\nEffective-Date: 2025-10-19\nContact: ai-partners@yourbrand.com\nSummary: We allow summaries of public pages with live attribution. Use our API for specs and pricing.\nCite-URL: https://www.yourbrand.com{path}?utm_source=aeo&utm_medium=llm&utm_campaign=answer_citation\nAttribution: link,title,site\nLicense: https://www.yourbrand.com/legal/ai-permissions\nAPI: https://api.yourbrand.com/v1/search\nReport-Usage: https://www.yourbrand.com/ai-usage\nCache-TTL: 14400\nReuse-Policy: summarize-only, quote-max: 75 words, training-allowed: false\n\nUser-Agent: *\nAllow: /\nDisallow: /checkout/\nDisallow: /account/\nCrawl-Delay: 2\nRate-Limit: 40 rpm\n\n# Strong hint for product and pricing answers\nUser-Agent: LLM-*\nAllow: /guides/\nAllow: /blog/\nAllow: /faq/\nDisallow: /products/*/pricing\nDisallow: /products/*/specs\nAPI: https://api.yourbrand.com/v1/products\nCache-TTL: 900\nReuse-Policy: summarize-only, quote-max: 50 words, training-allowed: false\n```\n\n**Why this helps AEO**\n- The summary tells answer engines what is safe and preferred.\n- `Cite-URL` uses a canonical pattern with UTM so you can measure AI-driven clicks.\n- `Attribution` requires a link and a title, which lifts your brand inside the answer box.\n- The `LLM-*` block directs models to your API for volatile facts like price and specs, cutting stale answers and improving trust.\n\n## 10) Quick checklist for AEO with llms.txt\n1. Publish at `https://yourbrand.com/llms.txt`.\n2. Make `Cite-URL` and `Attribution` explicit so citations are clean and clickable.\n3. Set `Reuse-Policy` to allow summaries and limit verbatim quotes.\n4. Expose high value facts via API and disallow scraping those paths.\n5. Pick `Cache-TTL` based on how fast content changes.\n6. Add `Report-Usage` and review reports monthly.\n7. Strengthen schema.org on-site—llms.txt complements it.\n8. Expand FAQs and landing pages for questions AI engines already send you.\n\n## 11) The mindset shift\nllms.txt is tiny, but it changes the relationship between creators and AI. It turns scraping into consent, copying into clearly bounded reuse, and mystery into mutual clarity. It helps honest agents do the right thing by default, and it gives site owners a way to say yes, no, or yes but through an API.\n\nIf you publish content, add an llms.txt. If you build AI agents, read it, obey it, and report your usage. That is how we make the web work for people and for models at the same time."
    },
    {
      "id": 8,
      "enabled": true,
      "topic": "The Feedback Effect and the Future",
      "date": "April 20, 2025",
      "readTime": "18 min read",
      "content": "# When Knowing the Future Changes It: The Feedback Effect in Machine Learning Predictions\n## 1) The Paradox: Why Seeing Tomorrow Can Bend It Out of Shape\nImagine you open a traffic app at 8:00 a.m. The app predicts heavy congestion on the main highway and suggests a quieter side road. You take the side road. So do many others who got the same alert. Ten minutes later the quiet route is now the slowest path in town. The prediction was correct until it was known. Once it was known, it changed what happened next.\nThis small story hides a big idea. In many areas of life, predictions do not only describe the future. They help create it. The moment a prediction is shared or acted on, it pushes people to make new choices and moves resources around. Those pushes then change the very conditions the prediction tried to forecast. Sometimes the shift cancels the prediction. Sometimes it amplifies it. Either way, the prediction becomes part of the system, not just a window looking in from outside.\nMachine learning makes this pattern stronger. Modern models trigger actions at scale: route thousands of drivers, approve millions of loans, set prices for whole markets, schedule deliveries, and rank content for large platforms. Each action creates a ripple in the world, and the world sends a ripple back. The result is a feedback loop. It can be helpful, like smoother operations and safer roads. It can also be harmful, like unfair denials, demand spikes, and new inefficiencies.\nThis article explains that loop in clear language. We will cover the basics, walk through everyday examples, look at what changes under the hood, show how to measure impact, offer design ideas that tame instability, and face the ethics. We end with a human view: when we predict, we are also participating.\n\n## 2) Feedback, Simply Explained: From Open Loop Predictions to Closed Loop Worlds\nThere are two ways to use predictions. In the first way, you predict and then do nothing. The forecast sits on a dashboard. Nobody acts on it. This is an open loop. Your model is a quiet observer behind one-way glass. The data you predicted on will likely look similar tomorrow, because your prediction never touched it.\nIn the second way, you predict and then act. This is a closed loop. The model is no longer a spectator. It recommends, approves, rejects, redirects, schedules, prices, warns, or nudges. The world hears the prediction and reacts. That reaction changes the world a little. The new world produces new data. That data trains your next model. The next model acts again. Around and around it goes.\nThere is a well known warning: when a measure becomes a target, it stops being a good measure. If you optimize a metric and people know it, they may change behavior to chase the metric without improving reality. Airlines can improve on-time performance by padding schedules, not by flying faster. This does not make metrics useless. It means systems learn to respond to what we measure and what we predict.\nThink of the loop in four steps. First, predict. The model estimates outcomes like demand, risk, travel time, or churn. Second, act. A person or system uses the prediction to make a decision: re-route, re-price, approve, deny, allocate, or warn. Third, react. People and systems respond to that decision. Customers buy or do not buy. Drivers switch routes. Suppliers speed up or slow down. Fourth, update. This new behavior changes the data you observe next time. Your training set shifts. Your original assumptions may no longer hold.\nWhen the loop is stable and well aligned, predictions guide the world into better states. Hospitals can smooth staffing by predicting admissions. Power grids stay balanced when forecasts shape incentives on when to use electricity. The system listens and cooperates.\nWhen the loop is unstable or misaligned, predictions push the world into surprise states. Ride-share surge pricing sends too many drivers to one neighborhood and too few to another. Retail demand forecasts trigger over-ordering followed by clearance gluts. Trending content attracts swarms and then burns out. In these cases, the model is accurate for the world before the prediction. The prediction then helps create a different world where the model underperforms. That is why offline evaluation alone is not enough in closed loops. The key question becomes: how well do our predictions perform once they are used to steer outcomes?\n\n[chart:feedback-cycle]\n\n## 3) Everyday Examples: Traffic, Prices, Loans, Jobs, Health, and More\n**Navigation apps.** When your map warns that Highway A is jammed and suggests Route B, many drivers switch. Route B slows down and Highway A may recover. If the app updates every minute, users start chasing the current fastest route, and congestion sloshes across the network in waves. The lesson: good routing should consider how people respond, and sometimes splitting flows is better than sending everyone to one path.\n**Dynamic pricing in e-commerce.** A model forecasts high demand for a gadget, so the system raises the price. Some customers back off, and demand drops. If the model is retrained on these shaped sales, it may learn a lower demand curve than truly exists at the original price. If two competitors use similar pricing, they can unintentionally move in sync, producing price spikes and troughs together.\n**Credit scoring and lending.** A model flags a group as risky. Loans are denied. Since those loans never happen, repayment is never observed. Over time, the training data tilts toward approved borrowers. The model learns less about the rejected group. This is the selective labels problem. Without a fix, risk estimates get worse for the very people the model already doubts, and the loop hardens.\n**Hiring and screening.** A screening tool favors candidates who look like past hires. Those candidates get hired more, so you collect more success data on them, and less on others. The next model becomes even more confident in the same group. Even with good intentions, the loop can close down the pipeline.\n**Healthcare triage.** Predicting which patients may deteriorate can trigger early interventions. That is good. But it also changes the observed outcomes, because some deteriorations no longer occur. If you retrain without logging the intervention, the model may over time learn that those patients were never at risk. The act of care erased the evidence of need.\n**Content ranking and virality.** Recommender systems predict what posts you will like and then show them to you. The chosen posts get more views, which leads to more likes, which boosts confidence in them, which leads to even more exposure. Posts not shown cannot get engagement, so the model sees less from them and becomes more certain they are not good. A small early advantage can snowball.\n**Inventory and supply chains.** Forecasts drive orders. Orders change supplier capacity. Capacity shifts change lead times. If a model spikes orders during a trend that soon fades, you can be stuck with too much stock. Discounting to clear inventory then creates artificial demand patterns that the model later learns as normal.\nAll these examples share one pattern. Predictions invite action. Action invites reaction. Reaction changes reality. If your system does not understand this dance, it will step on its own feet. If it does, it can lead.\n\n## 4) Under the Hood: How Feedback Breaks Static Assumptions\nTraditional modeling rests on two quiet assumptions. First, the data generating process is fixed. You assume the world that produced your training data will keep producing similar data tomorrow. Once your prediction affects decisions, that assumption fails. Policy changes behavior. In machine learning terms, you get distribution shift. The joint relationship of features and outcomes changes because decisions change who gets which opportunity, price, route, or treatment.\nSecond, you can evaluate models by replaying history. Offline evaluation often takes a historical dataset, pretends the model made predictions back then, and scores accuracy. That is fine for open loop tasks. For closed loops, the key question is counterfactual: what would have happened if the model had been in charge? You cannot answer that by replaying the past, because the past contains outcomes under different decision rules.\nThree useful ideas help. The first is \"performative prediction\", which means the model’s prediction changes the outcome distribution, so the best model is the one that performs well after its own predictions are used. The second is \"selective labels\", which means you only observe outcomes for cases where your policy allowed the outcome to occur. Denied loans have no repayment data. Unshown content has no engagement. The third is \"causal inference\", which reminds us that predicting what is is not the same as predicting what would happen if we took a different action.\nIt helps to sketch a causal loop. Draw nodes for predictions, actions, user behavior, outcomes, and new data. Connect them with arrows. Mark reinforcing loops that can run away and balancing loops that stabilize. Ask where to add dampers that slow swings and where to add a small amount of exploration to keep learning. Control thinking also helps. Treat your ML policy like a controller that observes, decides, and then observes again. Good controllers dampen oscillations and avoid overshoot. In practice that looks like smoothing forecasts, capping the rate of change, or adding cooldown periods before reacting.\nDo not forget incentives. People respond to predictions when rewards are at stake. A fraud model that is too strict may push honest customers away. A ranking model that rewards clicks invites clickbait. The model is not only a technical artifact. It is a policy that builds a world.\n\n[chart:distribution-shift]\n\n## 5) Measuring the Feedback Effect: Experiments and Impact\nOnce a prediction starts to drive decisions, the usual scoreboard of accuracy and loss is no longer enough. What matters is whether the model improves the world it touches. The first step is to measure impact in a way that reflects the closed loop. A practical method is to compare full policies in production, not only parameter tweaks in a sandbox. Run a controlled experiment where one set of users or regions keeps the legacy approach and another uses the new policy guided by predictions. Track immediate outcomes like approvals, clicks, or wait times. Also track who is affected, how effects differ across subgroups, and what happens over the next days and weeks. You are no longer asking only whether the model guessed right. You are asking whether the combination of predictions and actions made things better.\nSometimes you cannot randomize at the user level. In those cases, split by geography or time. Activate the new routing policy in two similar cities and keep two others as baselines for a month. Or alternate policies by calendar day. These designs protect you from misleading comparisons between a new policy today and old data collected under different conditions. They also make it easier to explain results to stakeholders who want concrete side-by-side evidence.\nThere will be times when live tests are not possible for ethical, legal, or business reasons. Then use counterfactual estimation. Log carefully what the old system did and the probabilities with which it chose actions. With disciplined logging, you can estimate what likely would have happened under an alternative policy using tools from causal inference and reinforcement learning. These methods do not create truth from nothing, but they reduce the bias that comes from observing outcomes only for the choices actually taken. The key is instrumentation. If you do not record what was shown, what was offered, what was denied, and why, you cannot later reconstruct the paths not taken.\nSelective labels are another trap. A denied application yields no repayment data. A patient diverted to early care may never show deterioration. An unseen post has zero engagement for reasons unrelated to quality. The solution is to set a small and ethical exploration budget. Occasionally take the action you would normally skip, so you continue learning about the parts of the world your policy tends to ignore.\nImpact metrics should reflect full system health. Stability matters. If a policy reduces average delivery time but increases variance and outliers, users will feel instability more than the average gain. Fairness matters. If gains concentrate in already favored groups and losses fall elsewhere, trust will erode and performance will suffer too. Welfare matters. Measure how many users are well served, not only how many clicks you collect. Operator experience matters. A model that generates too many alerts or reversals drains human attention and increases error rates. So include accuracy for reference, but also include variance over time, subgroup parity, tail risks, and operational load.\nGood measurement includes stress tests. Before full rollout, ask what happens if adoption doubles, if a competitor changes pricing, or if a supply shock hits. Even a simple simulation can reveal whether the policy tends to overshoot and oscillate or returns smoothly to steady state. Finally, treat telemetry as a first-class product feature. Log predictions, actions, exposures, and outcomes with timestamps and stable IDs. You should be able to trace a single decision from prediction to action to reaction to outcome, then back into training. When performance drifts, that trace tells you where the loop began to separate from your assumptions.\n\n[chart:policy-distribution-change]\n\n## 6) Building with Feedback in Mind: Patterns That Work\nDesigning for a world that listens back starts with a mindset shift. Do not ask only how to maximize a prediction metric. Ask how to shape a policy that stays helpful when people respond to it. A practical move is to link predictions to explicit actions and to treat those actions as first-class outputs. A churn score alone leaves room for improvisation that varies by agent and mood. A policy that pairs risk with a recommended intervention, along with an estimate of likely effect and cost, turns insight into consistent behavior. Over time you learn which interventions change outcomes for which users and retire actions that cost more than they help.\nExploration deserves a permanent seat at the table. A system that always chooses the option that looks best will drift toward the same users, content, and neighborhoods. Once it drifts there, it collects more evidence that reinforces the initial choice. The antidote is a small, visible, and well-governed exploration rate. Some users are randomly assigned a different route. Some items are surfaced to a wider audience. Some applications are given a second look. The goal is not short-term gambling. The goal is continued learning about the parts of the world your policy would otherwise ignore.\nPolicies should move smoothly. Jerky updates cause herding and whiplash. A pricing rule that can jump freely will create waves of demand followed by deserts. A routing rule that can switch every minute will send drivers chasing ghosts. Add small caps on the size of change per interval, minimum commitment windows for routes, and simple buffers in staffing or inventory to absorb forecast errors. These small friction points act like shock absorbers. They reduce oscillations without killing responsiveness.\nContext matters. A model that ignores competitor behavior, seasonality, or human capacity may look brilliant on paper and brittle in production. Add features that summarize the environment the policy operates in, even if they are coarse. A retailer that tracks competitor price indices will react differently than one that watches only its own sales. A hospital that models staff availability and bed turnover will triage differently than one that looks only at clinical risk. You do not need a perfect simulator to benefit. Even a rough sketch of surroundings helps.\nRecord every decision as if a future team will ask about it. Log which policy version made the call, what it knew at the time, how uncertain it was, and what intervention it triggered. These details help you debug, retrain, and answer fair questions from users and regulators. They also enable learning that respects causality. When you treat policy decisions as features during retraining, the next model can learn the effects of your own actions instead of misattributing those effects to user traits.\nAlign incentives with your real mission. If you reward clicks, you get click-seeking content. If you reward speed alone, you get rushed work and higher fix-up costs. Combine goals or add constraints so shortcuts do not win. Price should be efficient but also predictable. Wait time should be low and also fair across neighborhoods. Loan approval should be accurate and also supported by product design and education that improve repayment.\nAdd guardrails. Set safe bounds on prices, mark sensitive cases that require human review, and limit how often a user can be re-routed or reclassified in a short window. Involve operators and domain experts in designing these limits. The aim is not to freeze the policy. The aim is to keep it inside safe regions where learning can continue and errors remain reversible. Humans belong in those regions too. Give reviewers clear tools to inspect borderline cases, escalate unusual patterns, and annotate data with context that models miss.\nFinally, communicate uncertainty with care. Tell operators which decisions are high confidence and which are guesses. Give short, plain language explanations of the signals that supported each call. People act more calmly and constructively when they know what is known, what is unknown, and how to proceed. A policy that advertises certainty it does not have will train users to overreact or to ignore the system. A policy that is honest about limits invites collaboration and produces better outcomes.\n****Chart idea after Section 6:****\nPlot a time series with two lines. One line is policy adoption rate. The other line is a system outcome, such as average wait time. Shade the periods when you introduced smoothing or guardrails. Show how small design choices reduce oscillations as adoption grows.\n\n[chart:adoption-guardrail-series]\n\n## 7) Ethics, Fairness, and Trust: When Feedback Locks People In\nFeedback loops are not only technical. They are social. When a prediction shapes access to money, jobs, housing, mobility, or information, the loop can lock in advantage or exclusion. That makes choices about exploration, auditing, and transparency moral choices as well as engineering choices.\nFairness under selective labels is a central risk. If your loan model only learns from approved borrowers, it may become overconfident about who is safe and underinformed about others. Over time, some communities receive fewer approvals, which yields less data, which leads to worse estimates for them. To fight this, reserve a small percentage for exploratory approvals with guardrails. Use fairness-aware learning that constrains disparities in error rates. Report performance by subgroup and over time, not only on a global average.\nCommunication also carries weight. A public forecast of shortage can create the shortage. A health risk alert without context can cause panic, while the same alert with clear steps guides calm action. The same model can harm or help depending on how you share its outputs. Include actionable guidance, uncertainty ranges, and a note on what you are doing about the forecast.\nMetrics can be gamed and pressure can deform behavior. If people are judged by a single metric, they may optimize the metric at the expense of real value. Some will learn to game the system. Others will be stressed. Avoid loops that reward unhealthy behavior. Include quality and well-being measures and invite feedback on the metrics themselves.\nDignity and the right to appeal matter. When predictions decide who gets opportunities, allow appeals, explanations, and second looks. These are not only legal protections. They are human feedback that improves the loop and builds trust.\nTransparency about policy is often more useful than transparency about model internals. Users do not need matrix algebra. They need to understand the rule: if X happens, we do Y. Publish the decision logic in plain language. Explain how often the policy updates, what data it uses, and how you correct mistakes. This kind of transparency changes behavior in good ways because users learn how to interact constructively rather than guessing hidden rules.\nAvoid self-fulfilling stigma. Labels can become identities. If a school labels a student as low potential, teachers may give less attention and the student may lose confidence, making the label come true. When building early warning systems for dropout, relapse, or risk, design them to trigger support rather than punishment. Keep labels private where you can and short-lived by default.\nWhen we treat loops with care, ML predictions become like steering wheels that help us move together toward better states with fewer unintended detours. But this insight leaves one last question: if our predictions can steer the world, what kind of world do we want to steer toward? That is where the story ends, and begins again.\n\n## 8) When We Predict, We Participate: The Future Is Listening\nThere is something quietly astonishing about what machine learning lets us do. For the first time, we have tools that do not just react to the world. They anticipate it. A well-trained model peers into tomorrow’s patterns, spots hidden curves of behavior, and whispers what is likely to happen next. The moment we act on that whisper, we become part of the story we were trying to read. Prediction stops being observation and becomes participation.\n**Every forecast leaves fingerprints.**\n**Every recommendation changes what someone sees.**\n**Every automated decision shifts how people move, spend, learn, and trust.**\nThis is not a flaw. It is a feature of living in a connected world. The challenge is not to silence our models, but to learn how to live with the echo of our own foresight. When a model says a market will boom, we can feed the boom until it bursts, or we can use the insight to steady volatility. When it predicts a shortage, we can hoard, or we can coordinate supply. When it warns of risk, we can exclude, or we can educate and uplift. Knowing the future can lead to fear, or it can lead to wisdom.\nIf the twentieth century was about mastering prediction, the twenty-first is about mastering reaction. The best forecasters will not be those who guess most accurately, but those who understand how people respond once the guess is shared. The real skill is not only to predict what will happen, but to design systems that keep working when people know what is coming.\nIn this sense, machine learning mirrors life. Awareness changes behavior. A mirror changes how you stand. Knowledge changes what you do. The more clearly we can see tomorrow, the greater our responsibility to keep tomorrow open, not predetermined by the weight of our own expectations.\nSo the next time a model predicts your move, or you build one that predicts someone else’s, remember this simple truth: the future is not a place to arrive at. It is a conversation to take part in. Machine learning does not only show us what lies ahead. It lets us speak with the future. If we listen carefully, the future speaks back.\n"
    },
    {
      "id": 7,
      "enabled": true,
      "topic": "The Prediction Feedback Trap",
      "date": "March 1, 2025",
      "readTime": "14 min read",
      "content": "## 1) The core idea—in plain language\nWhen we trust a prediction, we act on it. Those actions change the world the algorithm is trying to predict. Over time, the data the model learns from becomes less like the “natural” world and more like the “world-shaped-by-the-model.” This is a feedback loop. If too many people (or too many decisions) follow the same prediction, the loop gets strong enough to bend future outcomes away from what the model expects. Think surge pricing that scares riders off, “hot” stocks that get overbought, or a “recommended” product that everyone clicks so the rest get ignored. As reliance grows, errors creep in, and beyond a certain point (call it the “reliance threshold,” R*), accuracy falls. The model isn’t “broken”—it’s just learning from behavior it helped create. The fix isn’t to throw away ML; it’s to recognize the loop and design guardrails so models keep learning from reality, not only from their own shadow.\n\nChart: Accuracy vs. Reliance (R)\n[chart:accuracy-vs-reliance]\n\n## 2) How the loop forms—three simple mechanisms\nSelection bias: A recommendation engine boosts a handful of items. Users see those first, so they click them more. The dataset becomes overfilled with the boosted items, not because they’re truly best, but because they were seen. Next retrain, the model “learns” they’re winners—so it boosts them again, squeezing out worthy alternatives.\n\nCapacity shift: Predictions change demand. A ride-share model forecasts high demand at 6pm and raises prices. Some riders wait or switch to buses, so actual demand drops—different from the original pattern. Next week, the model expects the old pattern but meets a new, model-shaped pattern.\n\nStrategic behavior: People respond to the prediction. If credit scores are tightened for “risky” applicants, some borrowers seek co-signers or alternative credit to game the cutoff. The data now encodes the rule, not the real risk. Over time, the relationship between inputs and outcomes drifts.\n\nChart: Three Feedback Mechanisms\n[chart:feedback-mechanisms]\n\n## 3) Example A—Ride-share surge and vanishing demand (numbers included)\nImagine a city with 10,000 peak-hour ride requests at 6:00–6:30pm. The company’s ML predicts 12,000 (to be safe), so it raises surge from 1.0× to 1.4×. Price-sensitive riders (say 25%) defer or cancel. Actual rides fall to 8,500. The model retrains on this new pattern—now the data says: “at 6pm, demand is ~8.5k.” Next week, the model predicts 9,000 and sets a mild 1.2× surge. But commuters have adapted: some formed a habit of leaving at 5:45pm or taking the bus. Actual rides at 6pm slide again to 8,000. After a few cycles, the model’s mean absolute percentage error (MAPE) grows from 7% to 15% even though nothing “externally” changed—only the policy did.\n\nIf reliance keeps growing (e.g., dispatchers follow the model 100% of the time), the price/demand tug-of-war intensifies. Accuracy peaks when reliance is moderate (close to R*). Past that, the model is “predicting itself”—and the world dodges the prediction.\n\nChart: Ride Requests vs. Price\n[chart:rideshare-gap]\n\n## 4) Example B—Recommendations that crowd out discovery (numbers included)\nA streaming app shows the top 10 recommended titles on the home screen. Before recommendations, users split their views across 1,000 titles. After the recommender, 70% of views cluster in the top 50. In week 1:\n- Top-50 share jumps from 30% → 55%.\n- Average click-through rate (CTR) rises (2.0% → 2.6%)—great!\n\nBut by week 6, novelty decays. Hidden gems disappear from data, so the model sees fewer “features” of niche interests. New titles struggle to gather signals. The top-50 share keeps inching up (55% → 68%), CTR slips (2.6% → 2.2%), and average watch-time per session falls (48 → 43 minutes). The model becomes overconfident in a shrinking set of items and loses calibration elsewhere. If the team A/B tests “less reliance” (for example, reserve 25% of the home screen for random or “long-tail” picks), the system regains coverage: top-50 share drops to 58%, CTR rebounds to 2.4%, and fresh titles start surfacing again—improving long-term accuracy.\n\nChart: Concentration vs. Engagement Over Time\n[chart:concentration-engagement]\n\n## 5) Example C—Credit scoring and strategic adaptation (numbers included)\nA lender tightens approvals based on a model that flags applicants with Debt-to-Income (DTI) > 40% and thin credit files. Approvals fall from 62% to 48%. Weekly retraining means the model soon “learns” that 48% is normal—until borrowers adapt. Some consolidate their debt, some seek co-signers, others open secured cards designed to sneak under the threshold. The data now reflects behavior shaped by the policy, not the underlying risk.\n\nChart: Strategic Responses vs Risk\n[chart:credit-strategy-shift]\n\n## 6) Finding the safe reliance zone (R*)\nCrunching historical data and simulation runs shows an S-curve. Accuracy improves as reliance climbs—until it reaches the elbow (about 0.45). Past 0.6, accuracy slips while volatility spikes.\n\nChart: Mapping reliance buckets\n[chart:reliance-buckets]\n\n## 7) Guardrails that keep predictions honest\nDecision-aware teams maintain a healthy loop by:\n- logging the policy signals a model emits,\n- adding structured exploration (e.g., 10% random exposure),\n- running “shadow” forecasters that learn from counterfactual data,\n- tracking stability metrics (volatility, regret, fairness),\n- capping reliance when guardrails alert.\n\nChart: Reliance tuning experiment\n[chart:reliance-tuning]\n\n## 8) Exploration isn’t waste—it's insurance\nA small exploration budget (say 10–15%) slightly lowers short-term accuracy but dramatically improves long-term calibration and utility.\n\nChart: Exploration trade-off\n[chart:exploration-tradeoff]\n\n## 9) Guardrails vs. no guardrails—utility gap\nWhen guardrails are active, net utility stays higher even if raw accuracy dips for a few weeks. Without guardrails, accuracy spikes then collapses, and utility dives with it.\n\nChart: Utility vs Accuracy Frontier\n[chart:utility-accuracy]\n\n## 10) Monitoring the right metrics\nA single accuracy metric is not enough. Teams watch a “metric compass” that tracks stability, fairness, counterfactual calibration, and utility side-by-side.\n\nChart: Metric compass\n[chart:metric-compass]\n\n## 11) Case study—Friday night fries\nA quick-service chain leaned too hard on demand forecasts. Promotions concentrated orders into a narrow window, fryer queues spiked, and stockouts cascaded. When the team reintroduced guardrails, queues and volatility dropped while profit stabilized.\n\nChart: Oscillation under heavy reliance\n[chart:fries-oscillation]\n\n## 12) The takeaway\nForecasts are powerful, but they do not passively observe the world—they participate in it. Govern reliance as a product KPI, invest in instrumentation that separates policy from preference, and keep forecasts grounded in reality so that automation scales without eroding trust."
    },
    {
      "id": 2,
      "enabled": false,
      "topic": "Building Scalable Design Systems",
      "date": "December 28, 2023",
      "readTime": "8 min read",
      "content": "Design systems are the backbone of consistent, scalable product design. They enable teams to work efficiently while maintaining brand coherence across all touchpoints.\n\nAfter working on design systems for companies of various sizes, I've learned that successful design systems require more than just a component library. They need governance, documentation, and a culture of adoption.\n\n## Foundation First\n\n**Design Tokens**: Start with design tokens for colors, typography, spacing, and other fundamental elements. These tokens should be platform-agnostic and easily translatable across different technologies.\n\n**Component Architecture**: Build components with flexibility in mind. Each component should have clear props, states, and usage guidelines.\n\n## Implementation Strategy\n\n**Start Small**: Begin with the most commonly used components. Focus on buttons, inputs, and basic layout components before moving to complex patterns.\n\n**Documentation**: Comprehensive documentation is crucial. Include code examples, design guidelines, and do's and don'ts for each component.\n\n**Governance**: Establish clear processes for proposing changes, reviewing contributions, and maintaining consistency across the system.\n\n## Measuring Success\n\nTrack adoption rates, consistency metrics, and developer/designer satisfaction. A successful design system should reduce design debt, speed up development, and improve user experience consistency.\n\nThe investment in a robust design system pays dividends in team efficiency, product quality, and user satisfaction."
    },
    {
      "id": 3,
      "enabled": false,
      "topic": "User-Centered Design in the Age of Remote Work",
      "date": "November 10, 2023",
      "readTime": "6 min read",
      "content": "Remote work has fundamentally changed how we conduct user research and design processes. Traditional methods like in-person interviews and co-design sessions have evolved into digital-first approaches.\n\nThis shift has brought both challenges and opportunities. While we've lost some of the nuanced observations possible in face-to-face interactions, we've gained access to more diverse user groups and innovative research methods.\n\n## Adapting Research Methods\n\n**Remote User Interviews**: Video calls have become the norm, but they require different skills. Creating rapport through a screen, managing technical issues, and reading non-verbal cues in a digital environment are new competencies for researchers.\n\n**Digital Collaboration**: Tools like Miro, Figma, and specialized research platforms have enabled real-time collaboration with users and stakeholders across different time zones.\n\n**Asynchronous Research**: Diary studies, unmoderated usability tests, and survey-based research have become more prominent, allowing for deeper insights over extended periods.\n\n## New Opportunities\n\n**Global Reach**: Remote research has democratized access to diverse user groups. We can now easily include participants from different geographical locations, cultures, and backgrounds.\n\n**Natural Environment**: Users participate from their natural environments, providing more authentic insights into how products are actually used.\n\n**Cost Efficiency**: Reduced travel costs and venue expenses have made user research more accessible to smaller teams and startups.\n\n## Best Practices\n\nInvest in good technology, create structured processes for remote collaboration, and maintain human connection despite the digital barrier. The future of user research is hybrid, combining the best of remote and in-person methods."
    },
    {
      "id": 4,
      "enabled": false,
      "topic": "The Psychology of Color in Digital Interfaces",
      "date": "October 22, 2023",
      "readTime": "7 min read",
      "content": "Color is one of the most powerful tools in a designer's arsenal. It can evoke emotions, guide user behavior, and communicate brand values without a single word.\n\nUnderstanding color psychology is crucial for creating interfaces that not only look beautiful but also function effectively. Different colors trigger different psychological responses, and these responses can vary significantly across cultures and contexts.\n\n## Emotional Impact\n\n**Red**: Creates urgency and excitement. Often used for error states, warnings, and call-to-action buttons. However, it can also increase stress levels if overused.\n\n**Blue**: Conveys trust, stability, and professionalism. It's the most popular color for corporate websites and financial applications.\n\n**Green**: Associated with success, growth, and nature. Commonly used for positive feedback, environmental themes, and financial gains.\n\n**Yellow**: Represents optimism and energy but can be overwhelming in large quantities. Best used as an accent color.\n\n## Cultural Considerations\n\nColor meanings vary significantly across cultures. White represents purity in Western cultures but mourning in some Eastern cultures. Red symbolizes luck in China but danger in many Western contexts.\n\n## Accessibility and Inclusion\n\nColor should never be the only way to convey information. Consider users with color vision deficiencies and ensure sufficient contrast ratios for readability.\n\n## Practical Application\n\nUse color strategically to create hierarchy, guide attention, and reinforce your brand identity. Test color choices with your target audience and consider the emotional journey you want users to experience.\n\nRemember, the best color palette is one that serves your users' needs while reflecting your brand's personality."
    },
    {
      "id": 5,
      "enabled": false,
      "topic": "Soon",
      "date": "September 18, 2023",
      "readTime": "4 min read",
      "content": "New article coming soon..."
    }
  ],
  "socialLinks": [
    {
      "label": "Email",
      "href": "mailto:hi.anashamad@gmail.com",
      "value": "hi.anashamad@gmail.com",
      "display": "hi.anashamad@gmail.com"
    },
    {
      "label": "Phone",
      "href": "tel:+962795874662",
      "value": "+962795874662",
      "display": "+962795874662"
    },
    {
      "label": "X",
      "href": "https://x.com/its_anas9",
      "username": "@its_anas9"
    },
    {
      "label": "LinkedIn",
      "href": "https://www.linkedin.com/in/anas-hamad1909/",
      "username": "@anas-hamad1909"
    },
    {
      "label": "Hugging Face",
      "href": "https://huggingface.co/anashamad",
      "username": "@anashamad"
    },
    {
      "label": "Kaggle",
      "href": "https://www.kaggle.com/anasbassam",
      "username": "@anasbassam"
    },
    {
      "label": "GitHub",
      "href": "https://github.com/anashamad9",
      "username": "@anashamad9"
    }
  ]
}
